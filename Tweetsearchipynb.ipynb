{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tweetsearchipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM5ICla3MAX5elphKfP9xbu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GaelleNovales/Trendr/blob/master/Tweetsearchipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0qJQsAjFpdI",
        "colab_type": "text"
      },
      "source": [
        "#Ressources"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M00xPdlfldlu",
        "colab_type": "text"
      },
      "source": [
        "##Téléchargement des ressources\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ur0DzztRvurI",
        "colab_type": "code",
        "outputId": "5166d0fd-6cbc-4993-d7aa-04aacff68237",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install nltk\n",
        "!pip install twint\n",
        "!pip install the Twython library \n",
        "!pip install emoji\n",
        "\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('sentiwordnet')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n",
            "Collecting twint\n",
            "  Downloading https://files.pythonhosted.org/packages/30/c8/0d09ef34ba23d38c7415d4f5b7de4c406f17b40b684ef6db30b8b0bdb362/twint-2.1.16.tar.gz\n",
            "Collecting aiohttp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/39/7eb5f98d24904e0f6d3edb505d4aa60e3ef83c0a58d6fe18244a51757247/aiohttp-3.6.2-cp36-cp36m-manylinux1_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 11.5MB/s \n",
            "\u001b[?25hCollecting aiodns\n",
            "  Downloading https://files.pythonhosted.org/packages/da/01/8f2d49b441573fd2478833bdba91cf0b853b4c750a1fbb9e98de1b94bb22/aiodns-2.0.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from twint) (4.6.3)\n",
            "Collecting cchardet\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/c5/7e1a0d7b4afd83d6f8de794fce82820ec4c5136c6d52e14000822681a842/cchardet-2.1.6-cp36-cp36m-manylinux2010_x86_64.whl (241kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 25.5MB/s \n",
            "\u001b[?25hCollecting elasticsearch\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cc/cf/7973ac58090b960857da04add0b345415bf1e1741beddf4cbe136b8ad174/elasticsearch-7.6.0-py2.py3-none-any.whl (88kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 10.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: pysocks in /usr/local/lib/python3.6/dist-packages (from twint) (1.7.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from twint) (0.25.3)\n",
            "Collecting aiohttp_socks\n",
            "  Downloading https://files.pythonhosted.org/packages/9d/a1/c9aed4d8f182c067a0314075a7e9e85ca917f0ecc831be92ddbf0719aef0/aiohttp_socks-0.3.6-py3-none-any.whl\n",
            "Collecting schedule\n",
            "  Downloading https://files.pythonhosted.org/packages/57/22/3a709462eb02412bd1145f6e53604f36bba191e3e4e397bea4a718fec38c/schedule-0.6.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: geopy in /usr/local/lib/python3.6/dist-packages (from twint) (1.17.0)\n",
            "Collecting fake-useragent\n",
            "  Downloading https://files.pythonhosted.org/packages/d1/79/af647635d6968e2deb57a208d309f6069d31cb138066d7e821e575112a80/fake-useragent-0.1.11.tar.gz\n",
            "Collecting googletransx\n",
            "  Downloading https://files.pythonhosted.org/packages/27/e1/77cd530afec7944d40c5bdd260bcc111be4012b045c82d4e3ffec90b2a42/googletransx-2.4.2.tar.gz\n",
            "Requirement already satisfied: typing-extensions>=3.6.5; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from aiohttp->twint) (3.6.6)\n",
            "Collecting idna-ssl>=1.0; python_version < \"3.7\"\n",
            "  Downloading https://files.pythonhosted.org/packages/46/03/07c4894aae38b0de52b52586b24bf189bb83e4ddabfe2e2c8f2419eec6f4/idna-ssl-1.1.0.tar.gz\n",
            "Collecting multidict<5.0,>=4.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/30/2e/3ab2f1fb72571f75013db323a3799d505d99f3bc203513604f1ffb9b7858/multidict-4.7.5-cp36-cp36m-manylinux1_x86_64.whl (148kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 29.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<4.0,>=2.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->twint) (3.0.4)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/8f/0209fc5d975f839344c33c822ff2f7ef80f6b1e984673a5a68f960bfa583/yarl-1.4.2-cp36-cp36m-manylinux1_x86_64.whl (252kB)\n",
            "\u001b[K     |████████████████████████████████| 256kB 27.0MB/s \n",
            "\u001b[?25hCollecting async-timeout<4.0,>=3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e1/1e/5a4441be21b0726c4464f3f23c8b19628372f606755a9d2e46c187e65ec4/async_timeout-3.0.1-py3-none-any.whl\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->twint) (19.3.0)\n",
            "Collecting pycares>=3.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/43/2d/7f4984a23f6e99cf6a8b20ddc59308efb209fe81e79c97af65e9b30eefae/pycares-3.1.1-cp36-cp36m-manylinux2010_x86_64.whl (228kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 29.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from aiodns->twint) (3.6.6)\n",
            "Requirement already satisfied: urllib3>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from elasticsearch->twint) (1.24.3)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->twint) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->twint) (2018.9)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from pandas->twint) (1.18.2)\n",
            "Requirement already satisfied: geographiclib<2,>=1.49 in /usr/local/lib/python3.6/dist-packages (from geopy->twint) (1.50)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from googletransx->twint) (2.21.0)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.6/dist-packages (from idna-ssl>=1.0; python_version < \"3.7\"->aiohttp->twint) (2.8)\n",
            "Requirement already satisfied: cffi>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pycares>=3.0.0->aiodns->twint) (1.14.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas->twint) (1.12.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->googletransx->twint) (2019.11.28)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=1.5.0->pycares>=3.0.0->aiodns->twint) (2.20)\n",
            "Building wheels for collected packages: twint, fake-useragent, googletransx, idna-ssl\n",
            "  Building wheel for twint (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for twint: filename=twint-2.1.16-cp36-none-any.whl size=33559 sha256=2c6dcb19307ec4031799642cc4cbaa12d424a6983486e0980b55559af2268fdd\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/0c/1d/2f876303c0c9c1cd3591a3fb0ec402081324964218390ef35b\n",
            "  Building wheel for fake-useragent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fake-useragent: filename=fake_useragent-0.1.11-cp36-none-any.whl size=13484 sha256=78877f03d0adf95355842eced39fbeceb5d522548568deeef891ebb8890bac34\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/63/09/d1dc15179f175357d3f5c00cbffbac37f9e8690d80545143ff\n",
            "  Building wheel for googletransx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletransx: filename=googletransx-2.4.2-cp36-none-any.whl size=15969 sha256=9cffd61ddec2b29a38d6103de8c115385370e9e68fb6030bff1fbb426897c1c1\n",
            "  Stored in directory: /root/.cache/pip/wheels/04/63/5f/75e7e94eb62517946116a783e4cd8970c4789c990bbc732616\n",
            "  Building wheel for idna-ssl (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for idna-ssl: filename=idna_ssl-1.1.0-cp36-none-any.whl size=3162 sha256=83977f23a8d3e353b8908c1aa38686a5946a39bf15a8fdce1664c3253d105a77\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/00/b3/32d613e19e08a739751dd6bf998cfed277728f8b2127ad4eb7\n",
            "Successfully built twint fake-useragent googletransx idna-ssl\n",
            "Installing collected packages: idna-ssl, multidict, yarl, async-timeout, aiohttp, pycares, aiodns, cchardet, elasticsearch, aiohttp-socks, schedule, fake-useragent, googletransx, twint\n",
            "Successfully installed aiodns-2.0.0 aiohttp-3.6.2 aiohttp-socks-0.3.6 async-timeout-3.0.1 cchardet-2.1.6 elasticsearch-7.6.0 fake-useragent-0.1.11 googletransx-2.4.2 idna-ssl-1.1.0 multidict-4.7.5 pycares-3.1.1 schedule-0.6.0 twint-2.1.16 yarl-1.4.2\n",
            "Collecting the\n",
            "  Downloading https://files.pythonhosted.org/packages/1e/39/f7d201c5afcccc4ed9654d0e47c57364a22fa5efd25fe9dd39d564541505/the-0.1.5.tar.gz\n",
            "Collecting Twython\n",
            "  Downloading https://files.pythonhosted.org/packages/8c/2b/c0883f05b03a8e87787d56395d6c89515fe7e0bf80abd3778b6bb3a6873f/twython-3.7.0.tar.gz\n",
            "Collecting library\n",
            "  Downloading https://files.pythonhosted.org/packages/1b/7b/5707105d93658b4abf9d3b38fe23205f18b9474c67ab01fe059043619a6b/Library-0.0.0.tar.gz\n",
            "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from Twython) (2.21.0)\n",
            "Requirement already satisfied: requests_oauthlib>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from Twython) (1.3.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.1.0->Twython) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.1.0->Twython) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.1.0->Twython) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.1.0->Twython) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests_oauthlib>=0.4.0->Twython) (3.1.0)\n",
            "Building wheels for collected packages: the, Twython, library\n",
            "  Building wheel for the (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for the: filename=the-0.1.5-cp36-none-any.whl size=6006 sha256=6fd0bd7cece6ca2326c4287c1d15c5f1d307ca556b15e74686f4e95898342bae\n",
            "  Stored in directory: /root/.cache/pip/wheels/ee/24/62/e4dcee49ca313b9f6a530fa9778777894ce9062092cda7b639\n",
            "  Building wheel for Twython (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Twython: filename=twython-3.7.0-cp36-none-any.whl size=31996 sha256=c45cf8f205239f2a96aa8231137bbac803d052864a3ed0218623ddc283e048f9\n",
            "  Stored in directory: /root/.cache/pip/wheels/c2/b0/a3/5c4b4b87b8c9e4d99f1494a0b471f0134a74e5fb33d426d009\n",
            "  Building wheel for library (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for library: filename=Library-0.0.0-cp36-none-any.whl size=2088 sha256=ed507a9a96bf158d92a1051bf141c3d3640521e78a2d1fdcac4d56e5464bd689\n",
            "  Stored in directory: /root/.cache/pip/wheels/08/4a/c2/ab9e600e97ee6c0acd29a67304cd3acd7f1760b47ce3134dfe\n",
            "Successfully built the Twython library\n",
            "Installing collected packages: the, Twython, library\n",
            "Successfully installed Twython-3.7.0 library-0.0.0 the-0.1.5\n",
            "Collecting emoji\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/8d/521be7f0091fe0f2ae690cc044faf43e3445e0ff33c574eae752dd7e39fa/emoji-0.5.4.tar.gz (43kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 2.6MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-0.5.4-cp36-none-any.whl size=42176 sha256=0dc55ba9305780c936799f213d10258e221d52fedb58a6ecb01a87aa329f78f5\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/a9/0a/4f8e8cce8074232aba240caca3fade315bb49fac68808d1a9c\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-0.5.4\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package sentiwordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/sentiwordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqQIoDNbFt34",
        "colab_type": "text"
      },
      "source": [
        "##Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZ53eN4lFwiS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import twint\n",
        "from datetime import datetime\n",
        "import emoji\n",
        "import regex\n",
        "import re\n",
        "from nltk import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import WordNetLemmatizer\n",
        "import pandas as pd\n",
        "from textblob import TextBlob\n",
        "import csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phEOj3fplrft",
        "colab_type": "text"
      },
      "source": [
        "#Création du projet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hg1KSWr6li7C",
        "colab_type": "text"
      },
      "source": [
        "## Importation des tweets selon un hashtag\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmZfM-Cw8pTJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def export_tweets(mention,taille):\n",
        "  date = datetime.now()\n",
        "  date_format = str(date.year)+'-'+str(date.month)+'-'+str(date.day)\n",
        "\n",
        "  # Configure\n",
        "  c = twint.Config()\n",
        "  c.Search = mention\n",
        "  c.Store_csv = True\n",
        "  c.Output = c.Search+\"_saved.csv\"\n",
        "  c.Limit = taille\n",
        "  c.Since = date_format\n",
        "  c.Lang = 'en'\n",
        "  c.Pandas = True\n",
        "\n",
        "  # Run\n",
        "  twint.run.Search(c)\n",
        "\n",
        "  twint_data = twint.storage.panda.Tweets_df\n",
        "  return twint_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4l7hS7Cl0yU",
        "colab_type": "text"
      },
      "source": [
        "##Extraction des emojis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xH2BnijOHOpT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_emojis(a_list):\n",
        "    emojis_list = map(lambda x: ''.join(x.split()), emoji.UNICODE_EMOJI.keys())\n",
        "    r = re.compile('|'.join(re.escape(p) for p in emojis_list))\n",
        "    aux=[' '.join(r.findall(s)) for s in a_list]\n",
        "    return(aux)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKUvgG6DI4f5",
        "colab_type": "text"
      },
      "source": [
        "##Traduire les abréviations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6j8mK4uHHjnW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translator(user_string):\n",
        "    user_string = user_string.split(\" \")\n",
        "    j = 0\n",
        "    for _str in user_string:\n",
        "        # File path which consists of Abbreviations.\n",
        "        fileName = \"slang.txt\"\n",
        "        # File Access mode [Read Mode]\n",
        "        accessMode = \"r\"\n",
        "        with open(fileName, accessMode) as myCSVfile:\n",
        "            # Reading file as CSV with delimiter as \"=\", so that abbreviation are stored in row[0] and phrases in row[1]\n",
        "            dataFromFile = csv.reader(myCSVfile, delimiter=\"=\")\n",
        "            # Removing Special Characters.\n",
        "            _str = re.sub('[^a-zA-Z0-9-_.]', '', _str)\n",
        "            for row in dataFromFile:\n",
        "                # Check if selected word matches short forms[LHS] in text file.\n",
        "                if _str.upper() == row[0]:\n",
        "                    # If match found replace it with its appropriate phrase in text file.\n",
        "                    user_string[j] = row[1]\n",
        "            myCSVfile.close()\n",
        "        j = j + 1\n",
        "    # Replacing commas with spaces for final output.\n",
        "    return (' '.join(user_string))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aVmkbRAl6x_",
        "colab_type": "text"
      },
      "source": [
        "##Cleaning du texte"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHU8nxJ5jhbO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    This function takes as input a text on which several \n",
        "    NLTK algorithms will be applied in order to preprocess it\n",
        "    \"\"\"\n",
        "    # Remplacer les abréviations\n",
        "    #print(text)\n",
        "    #print(translator(text))\n",
        "    text = translator(text)\n",
        "\n",
        "    tokens = word_tokenize(text)\n",
        "      \n",
        "    # Remove the punctuations\n",
        "    tokens = [word for word in tokens if word.isalpha()]\n",
        "    # Lower the tokens\n",
        "    tokens = [word.lower() for word in tokens]\n",
        "\n",
        "    # Remove stopword\n",
        "    tokens = [word for word in tokens if not word in stopwords.words(\"english\")]\n",
        "\n",
        "    # Corriger les fautes\n",
        "    tokens = [str(TextBlob(word).correct()) for word in tokens]\n",
        "\n",
        "    # Lemmatize\n",
        "    lemma = WordNetLemmatizer()\n",
        "    tokens = [lemma.lemmatize(word, pos = \"v\") for word in tokens]\n",
        "    tokens = [lemma.lemmatize(word, pos = \"n\") for word in tokens]\n",
        "    return tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Vxy-dldGmkm",
        "colab_type": "text"
      },
      "source": [
        "#Execution du programme"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dyqn76KUGiQ8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prog(mention,taille):\n",
        "  Tweets_df = export_tweets(mention,taille)\n",
        "\n",
        "  tweet_cleaned = []\n",
        "  for i in range(len(Tweets_df)):\n",
        "    tweet_cleaned.append(clean_text(Tweets_df['tweet'][i]))  \n",
        "\n",
        "  emojisExtracted = extract_emojis(Tweets_df['tweet'])\n",
        "  Tweets_df[\"tweet_emojis\"] = emojisExtracted\n",
        "  Tweets_df[\"tweet_cleaned\"] = tweet_cleaned\n",
        "  return Tweets_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMG5fRSmIEVh",
        "colab_type": "code",
        "outputId": "569d37eb-ab00-4983-d6d0-bb977011f2e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 700
        }
      },
      "source": [
        "Tweets_df = prog(\"#ok\",10)\n",
        "#df.drop('column_name', axis=1, inplace=True)\n",
        "Tweets_df"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1244601539777495041 2020-03-30 12:25:05 UTC <DannysCarSale> Come test drive this 2008 Lincoln MKX! Get great mpg (city 16/ hwy 24) with only 160+ miles on the odometer! Find solace in ZERO accidents and a well documented service history! Call (580) 233-2277 today! #DannysCarSales #Enid #OK  pic.twitter.com/qUhwUJokEm\n",
            "1244587774134693890 2020-03-30 11:30:23 UTC <BearWillLife> There's no better time than the present ! Individual, Team, and Family Coaching   http://ow.ly/uWgv50yZjL8  Wake Up ! #Lifecoah #Coach #TX #OK #NY #CA #Motivate #Determination #Improve #Life #DISC #Team #Teambuilding #Extreme #Execution #ExtremeExecution #CertifiedCoach #Chef #Food pic.twitter.com/E2piqGRSOb\n",
            "1244576664505348096 2020-03-30 10:46:14 UTC <MayankYdv3> Gs que. Change  Most spoken word in the world before #OK Now #corona\n",
            "1244575319912308737 2020-03-30 10:40:54 UTC <RobertPreuss>  https://www.technologyreview.com/s/615370/coronavirus-pandemic-social-distancing-18-months/ … OK. OK? #ok\n",
            "1244571836370870273 2020-03-30 10:27:03 UTC <Ryunicity> ‘mkay unnie! And have done do that. Maybe become tired and dizzy even more but all are #OK! 😘 Thanks for your concern!\n",
            "1244568414670204928 2020-03-30 10:13:27 UTC <Sj11041630> It's ok to be not #ok\n",
            "1244565480372727809 2020-03-30 10:01:48 UTC <VigarooPlus> Oklahoma Marijuana Dispensaries Can Offer Curbside Pickup  https://goo.gl/pY7T1N  #OklahomaCity #OK #Property\n",
            "1244549496790343681 2020-03-30 08:58:17 UTC <KarenDhaliwal2> Smiling is not to fake what’s going on in the world. Chaos is going to happen, but as a LEADER I’m going to smile through it anyway because my attitude in the crisis is the lesson to the person who is watching me. #COVID19 #Leaders #attitude #positivity #Outlook #community #Ok  pic.twitter.com/V2o4mZNuYW\n",
            "1244538003302813698 2020-03-30 08:12:37 UTC <1993FreedomAct> #FacialRecognition #Payments #AllWorldCantBuySellOr #CheckOut #WithOutIt #ItsNowIn #China #And #Russia=Next #Look  https://www.biblegateway.com/passage/?search=Revelation+13%3A16-17&version=KJV … #RightInBible #EachCant #CheckOut #Buy #Or #Sell #Without #Govt #NowBy #AI #EWallets #InNations #Thats #AI #In #Banking #OK #NowDoUBelieveGod https://twitter.com/TDKCDRW4x1/status/1241778873915883521 …\n",
            "1244524926687744006 2020-03-30 07:20:39 UTC <shellyd1967> My mum is 74 lives on Anglesey with no family there she also as copd , and no internet I’m desperate to see her too but unlike @SKinnock I won’t be a pillock #its #ok #im a MP\n",
            "1244523662616608773 2020-03-30 07:15:37 UTC <dmja6500> twitter began. it is very well .  #ok\n",
            "1244522264088825856 2020-03-30 07:10:04 UTC <QuakesToday> 2.7 magnitude #earthquake. 2 km from Guthrie, #OK, United States  http://www.earthquaketrack.com/quakes/2020-03-30-07-04-17-utc-2-7-2 …\n",
            "1244519657329098752 2020-03-30 06:59:43 UTC <billslater> . #Yuge!  I think #JudyGarland nailed this in #TheWizardOfOz in 1939:  #LionsAndTigersAndBearsOhMy!  #justsaying #TigerKing #OK #Oklahoma pic.twitter.com/npuHRHyBmk\n",
            "1244513820334317570 2020-03-30 06:36:31 UTC <BearWillLife> Success is yours !   Individual, Team, and Family Coaching   http://ow.ly/RMzR50yZjIu  Wake Up ! #Lifecoah #Coach #TX #OK #NY #CA #Motivate #Determination #Improve #Life #DISC #Team #Teambuilding #Extreme #Execution #ExtremeExecution #CertifiedCoach #Chef #Food pic.twitter.com/QZAZyLV3UX\n",
            "1244513239507111936 2020-03-30 06:34:12 UTC <BearWillLife> Stay and remain empowered   Individual, Team, and Family Coaching   http://ow.ly/bFRu50yZjEc  Wake Up ! #Lifecoah #Coach #TX #OK #NY #CA #Motivate #Determination #Improve #Life #DISC #Team #Teambuilding #Extreme #Execution #ExtremeExecution #CertifiedCoach #Chef #Food pic.twitter.com/4Id0v02l2p\n",
            "1244503357261090816 2020-03-30 05:54:56 UTC <davidcoverdale> I’m Serious!!!  #ButILoveCake #WhatAboutYourStagePants #TheyreTight #WayTight #YouDontWantThatCake #ok  pic.twitter.com/jnqaATzHwM\n",
            "1244497523340750849 2020-03-30 05:31:45 UTC <VigarooPlus> Stitt Orders Travelers from New York, Other COVID-19 'Hot Spots' Quarantined  https://goo.gl/pY7T1N  #OklahomaCity #OK #Property\n",
            "1244492107923914752 2020-03-30 05:10:14 UTC <LanCountryMusic> -2,489 deaths in: #NY #WA #LA #NJ #GA #CA #MI #IL #MA #CT #IN #TX #CO #NV #FL #VT #MO #WI #AZ #DC #MD #OH #OR #PA #PR #TN #VA #AR #HI #KY #ID #MS #GU #WV #AL #NE #OK #MN https://twitter.com/CEDRdigital/status/1244474657841000448 …\n",
            "1244489804093566976 2020-03-30 05:01:05 UTC <yunggdejj_> i’m pissed. they tryna hate on taurus. ok #ok\n",
            "1244488109661839361 2020-03-30 04:54:21 UTC <WorldNewsApp1> As #NewYorkCity quickly becomes the #epicenter of the #coronaviruspandemic in the #UnitedStates , the city is experiencing a shortage most people are #OK with: cats and dogs. The city's animal shelters are…  https://www.instagram.com/p/B-WFhzIhHZI/?igshid=123t8uyy1zu3e …\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-373ec9ec8f0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mTweets_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"#ok\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#df.drop('column_name', axis=1, inplace=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mTweets_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-5cd4c769ee89>\u001b[0m in \u001b[0;36mprog\u001b[0;34m(mention, taille)\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mtweet_cleaned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTweets_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtweet_cleaned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTweets_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tweet'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0memojisExtracted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_emojis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTweets_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tweet'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-80a2e3a78036>\u001b[0m in \u001b[0;36mclean_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m#print(text)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m#print(translator(text))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranslator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-0154b212594a>\u001b[0m in \u001b[0;36mtranslator\u001b[0;34m(user_string)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;31m# File Access mode [Read Mode]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0maccessMode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccessMode\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmyCSVfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0;31m# Reading file as CSV with delimiter as \"=\", so that abbreviation are stored in row[0] and phrases in row[1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mdataFromFile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyCSVfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"=\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'slang.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WeE0IZPsGkF9",
        "colab_type": "text"
      },
      "source": [
        "#Idées"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBKIFhQ3OQ4C",
        "colab_type": "text"
      },
      "source": [
        "##Correction de sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHek8CpNOQIq",
        "colab_type": "code",
        "outputId": "7a7c8a4d-441c-4e24-b776-74c30a139eba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "b = TextBlob(\"I havv goood speling!\")\n",
        "print(b.correct())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I have good spelling!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YoRPZVCoSvZ",
        "colab_type": "text"
      },
      "source": [
        "##Tokénisation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IszlYr1oDTl",
        "colab_type": "code",
        "outputId": "7178e657-3e4e-48ec-beed-c549a2471150",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "nltk.pos_tag(['go'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('go', 'VB')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQyMbOQPqWuO",
        "colab_type": "text"
      },
      "source": [
        "##Emotions *par mot*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3q9LKMjSqYjN",
        "colab_type": "code",
        "outputId": "214b148b-a8eb-471b-cd20-83ce08c420c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "from nltk.corpus import sentiwordnet as swn\n",
        "list(swn.senti_synsets('love'))\n",
        "\n",
        "breakdown = swn.senti_synset('kill.v.01')\n",
        "print(\"pos_score: \" , breakdown.pos_score())\n",
        "print(\"neg_score: \" , breakdown.neg_score())\n",
        "print(\"obj_score: \" , breakdown.obj_score())\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pos_score:  0.0\n",
            "neg_score:  0.5\n",
            "obj_score:  0.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3gssa1HmFq3",
        "colab_type": "text"
      },
      "source": [
        "##2eme essai de cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MvbyHZ7C0Lv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize \n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import LancasterStemmer \n",
        "\n",
        "#create an object of class PorterStemmer\n",
        "stop_words = set(stopwords.words('english'))   \n",
        "\n",
        "for i in range(len(Tweets_df)):\n",
        "  word_tokens = word_tokenize(Tweets_df['tweet'][i])   \n",
        "  filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
        "  filtered_sentence = []   \n",
        "  for w in word_tokens: \n",
        "      if w not in stop_words: \n",
        "          filtered_sentence.append(w) \n",
        "  #print(word_tokens) \n",
        "  #print(filtered_sentence)\n",
        "  tweet_rmStop.append(filtered_sentence)\n",
        "  \n",
        "tweet_lemm = tweet_rmStop"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEfDmBTbFHyi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import these modules \n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import LancasterStemmer\n",
        "\n",
        "#create an object of class PorterStemmer\n",
        "porter = PorterStemmer()\n",
        "lancaster=LancasterStemmer()\n",
        "\n",
        "for i in range(len(tweet_rmStop)):\n",
        "  for j in range(len(tweet_rmStop[i])):\n",
        "    tweet_lemm[i][j] = porter.stem(tweet_rmStop[i][j])\n",
        "    print(tweet_lemm[i][j])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUdp7SOEJ62Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas\n",
        "#Tweets_df = pandas.DataFrame()\n",
        "\n",
        "Tweets_df[\"tweet_rmStop\"]= tweet_rmStop\n",
        "Tweets_df[\"tweet_lemm\"] = tweet_lemm\n",
        "Tweets_df[\"tweet_emojis\"] = emojisExtracted"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}