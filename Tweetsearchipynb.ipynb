{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tweetsearchipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOVM37mwSAfoixeoMmA+Dq8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GaelleNovales/Trendr/blob/master/Tweetsearchipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0qJQsAjFpdI",
        "colab_type": "text"
      },
      "source": [
        "#Ressources"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M00xPdlfldlu",
        "colab_type": "text"
      },
      "source": [
        "##T√©l√©chargement des ressources\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ur0DzztRvurI",
        "colab_type": "code",
        "outputId": "f4d3c98c-3f02-4acf-ce39-389ff677d0dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 976
        }
      },
      "source": [
        "!pip install nltk\n",
        "!pip install twint\n",
        "!pip install the Twython library \n",
        "!pip install emoji\n",
        "\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('sentiwordnet')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n",
            "Requirement already satisfied: twint in /usr/local/lib/python3.6/dist-packages (2.1.16)\n",
            "Requirement already satisfied: cchardet in /usr/local/lib/python3.6/dist-packages (from twint) (2.1.6)\n",
            "Requirement already satisfied: pysocks in /usr/local/lib/python3.6/dist-packages (from twint) (1.7.1)\n",
            "Requirement already satisfied: aiodns in /usr/local/lib/python3.6/dist-packages (from twint) (2.0.0)\n",
            "Requirement already satisfied: elasticsearch in /usr/local/lib/python3.6/dist-packages (from twint) (7.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.6/dist-packages (from twint) (3.6.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from twint) (0.25.3)\n",
            "Requirement already satisfied: schedule in /usr/local/lib/python3.6/dist-packages (from twint) (0.6.0)\n",
            "Requirement already satisfied: googletransx in /usr/local/lib/python3.6/dist-packages (from twint) (2.4.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from twint) (4.6.3)\n",
            "Requirement already satisfied: geopy in /usr/local/lib/python3.6/dist-packages (from twint) (1.17.0)\n",
            "Requirement already satisfied: aiohttp-socks in /usr/local/lib/python3.6/dist-packages (from twint) (0.3.6)\n",
            "Requirement already satisfied: fake-useragent in /usr/local/lib/python3.6/dist-packages (from twint) (0.1.11)\n",
            "Requirement already satisfied: typing; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from aiodns->twint) (3.6.6)\n",
            "Requirement already satisfied: pycares>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from aiodns->twint) (3.1.1)\n",
            "Requirement already satisfied: urllib3>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from elasticsearch->twint) (1.24.3)\n",
            "Requirement already satisfied: chardet<4.0,>=2.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->twint) (3.0.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->twint) (19.3.0)\n",
            "Requirement already satisfied: async-timeout<4.0,>=3.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->twint) (3.0.1)\n",
            "Requirement already satisfied: multidict<5.0,>=4.5 in /usr/local/lib/python3.6/dist-packages (from aiohttp->twint) (4.7.5)\n",
            "Requirement already satisfied: typing-extensions>=3.6.5; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from aiohttp->twint) (3.6.6)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->twint) (1.4.2)\n",
            "Requirement already satisfied: idna-ssl>=1.0; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from aiohttp->twint) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from pandas->twint) (1.18.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->twint) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->twint) (2.8.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from googletransx->twint) (2.21.0)\n",
            "Requirement already satisfied: geographiclib<2,>=1.49 in /usr/local/lib/python3.6/dist-packages (from geopy->twint) (1.50)\n",
            "Requirement already satisfied: cffi>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pycares>=3.0.0->aiodns->twint) (1.14.0)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.6/dist-packages (from yarl<2.0,>=1.0->aiohttp->twint) (2.8)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas->twint) (1.12.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->googletransx->twint) (2019.11.28)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=1.5.0->pycares>=3.0.0->aiodns->twint) (2.20)\n",
            "Requirement already satisfied: the in /usr/local/lib/python3.6/dist-packages (0.1.5)\n",
            "Requirement already satisfied: Twython in /usr/local/lib/python3.6/dist-packages (3.7.0)\n",
            "Requirement already satisfied: library in /usr/local/lib/python3.6/dist-packages (0.0.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from Twython) (1.3.0)\n",
            "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from Twython) (2.21.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.4.0->Twython) (3.1.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.1.0->Twython) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.1.0->Twython) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.1.0->Twython) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.1.0->Twython) (2019.11.28)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.6/dist-packages (0.5.4)\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package sentiwordnet to /root/nltk_data...\n",
            "[nltk_data]   Package sentiwordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqQIoDNbFt34",
        "colab_type": "text"
      },
      "source": [
        "##Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZ53eN4lFwiS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import twint\n",
        "from datetime import datetime\n",
        "import emoji\n",
        "import regex\n",
        "import re\n",
        "from nltk import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import WordNetLemmatizer\n",
        "import pandas as pd\n",
        "from textblob import TextBlob\n",
        "import csv\n",
        "from datetime import date, datetime, timedelta\n",
        "from collections import Counter\n",
        "import json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phEOj3fplrft",
        "colab_type": "text"
      },
      "source": [
        "#Cr√©ation du projet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hg1KSWr6li7C",
        "colab_type": "text"
      },
      "source": [
        "## Importation des tweets selon un hashtag\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmZfM-Cw8pTJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def export_tweets(mention,taille):\n",
        "  date = datetime.now()\n",
        "  date_format = str(date.year)+'-'+str(date.month)+'-'+str(date.day)\n",
        "  \n",
        "  date_end = date - timedelta(days=7)\n",
        "  date_end_format = str(date_end.year)+'-'+str(date_end.month)+'-'+str(date_end.day)\n",
        "\n",
        "  # Configure\n",
        "  c = twint.Config()\n",
        "  c.Search = mention\n",
        "  c.Store_csv = True\n",
        "  c.Output = c.Search+\"_saved.csv\"\n",
        "  c.Limit = taille\n",
        "  c.Since = date_end_format\n",
        "  c.Until = date_format\n",
        "  c.Lang = 'en'\n",
        "  c.Pandas = True\n",
        "\n",
        "  # Run\n",
        "  twint.run.Search(c)\n",
        "\n",
        "  twint_data = twint.storage.panda.Tweets_df\n",
        "  return twint_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4l7hS7Cl0yU",
        "colab_type": "text"
      },
      "source": [
        "##Extraction des emojis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xH2BnijOHOpT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_emojis(a_list):\n",
        "    emojis_list = map(lambda x: ''.join(x.split()), emoji.UNICODE_EMOJI.keys())\n",
        "    r = re.compile('|'.join(re.escape(p) for p in emojis_list))\n",
        "    aux=[' '.join(r.findall(s)) for s in a_list]\n",
        "    return(aux)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKUvgG6DI4f5",
        "colab_type": "text"
      },
      "source": [
        "##Traduire les abr√©viations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6j8mK4uHHjnW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translator(user_string):\n",
        "    user_string = user_string.split(\" \")\n",
        "    j = 0\n",
        "    for _str in user_string:\n",
        "        # File path which consists of Abbreviations.\n",
        "        fileName = \"slang.txt\"\n",
        "        # File Access mode [Read Mode]\n",
        "        accessMode = \"r\"\n",
        "        with open(fileName, accessMode) as myCSVfile:\n",
        "            # Reading file as CSV with delimiter as \"=\", so that abbreviation are stored in row[0] and phrases in row[1]\n",
        "            dataFromFile = csv.reader(myCSVfile, delimiter=\"=\")\n",
        "            # Removing Special Characters.\n",
        "            _str = re.sub('[^a-zA-Z0-9-_.]', '', _str)\n",
        "            for row in dataFromFile:\n",
        "                # Check if selected word matches short forms[LHS] in text file.\n",
        "                if _str.upper() == row[0]:\n",
        "                    # If match found replace it with its appropriate phrase in text file.\n",
        "                    user_string[j] = row[1]\n",
        "            myCSVfile.close()\n",
        "        j = j + 1\n",
        "    # Replacing commas with spaces for final output.\n",
        "    return (' '.join(user_string))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aVmkbRAl6x_",
        "colab_type": "text"
      },
      "source": [
        "##Cleaning du texte"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHU8nxJ5jhbO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_text(text,mention):\n",
        "    \"\"\"\n",
        "    This function takes as input a text on which several \n",
        "    NLTK algorithms will be applied in order to preprocess it\n",
        "    \"\"\"\n",
        "    # Remplacer les abr√©viations\n",
        "    #print(text)\n",
        "    #print(translator(text))\n",
        "    text = translator(text)\n",
        "\n",
        "    tokens = word_tokenize(text)\n",
        "      \n",
        "    # Remove the punctuations\n",
        "    tokens = [word for word in tokens if word.isalpha()]\n",
        "    # Lower the tokens\n",
        "    tokens = [word.lower() for word in tokens]\n",
        "\n",
        "    # Remove stopword\n",
        "    tokens = [word for word in tokens if not word in stopwords.words(\"english\")]\n",
        "\n",
        "    # Corriger les fautes\n",
        "    #for word in tokens:\n",
        "    #  print(word)\n",
        "    #  print(str(TextBlob(word).correct()))\n",
        "    tokens = [str(TextBlob(word).correct()) for word in tokens if word != mention]\n",
        "\n",
        "    # Lemmatize\n",
        "    lemma = WordNetLemmatizer()\n",
        "    tokens = [lemma.lemmatize(word, pos = \"v\") for word in tokens]\n",
        "    tokens = [lemma.lemmatize(word, pos = \"n\") for word in tokens]\n",
        "    return tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Vxy-dldGmkm",
        "colab_type": "text"
      },
      "source": [
        "#Execution du programme"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dyqn76KUGiQ8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prog(mention,taille):\n",
        "  Tweets_df = export_tweets(mention,taille)\n",
        "\n",
        "  tweet_cleaned = []\n",
        "  for i in range(len(Tweets_df)):\n",
        "    tweet_cleaned.append(clean_text(Tweets_df['tweet'][i],mention))  \n",
        "\n",
        "  emojisExtracted = extract_emojis(Tweets_df['tweet'])\n",
        "  Tweets_df[\"tweet_emojis\"] = emojisExtracted\n",
        "  Tweets_df[\"tweet_cleaned\"] = tweet_cleaned\n",
        "  return Tweets_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMG5fRSmIEVh",
        "colab_type": "code",
        "outputId": "20cfc528-c938-4e54-a839-7f6c3733701e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "source": [
        "Tweets_df = prog(\"pepsi\",10)\n",
        "#df.drop('column_name', axis=1, inplace=True)"
      ],
      "execution_count": 282,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1244413868258070528 2020-03-29 23:59:20 UTC <BrianLikesToes> But I want Pepsi üò°\n",
            "1244413826331824129 2020-03-29 23:59:10 UTC <cryptogotham> Polar bears only live in the North. Coca Cola is from Atlanta, Georgia.   Signed- Pepsi and üêß  pic.twitter.com/cQOB3ryJgy\n",
            "1244413543430135808 2020-03-29 23:58:03 UTC <BapakayeEvans> Bros won drink Pepsi\n",
            "1244413037018329091 2020-03-29 23:56:02 UTC <ItsHailen> I remember about a year ago. I got an old clone trooper helmet from a thrift shop. Painted it then went on omegle to fuck with people with it. I pretended to drink soda with the helmet on only for my penis to be soaked in Pepsi. I kept doing that bit over and over. Best night.  pic.twitter.com/rgGgDSsZM8\n",
            "1244412990499303425 2020-03-29 23:55:51 UTC <hardehunhmi> I bind and I cast every spirit of Pepsi..... I can't sleepüò≠üò≠\n",
            "1244412591587430401 2020-03-29 23:54:16 UTC <ChitownKev> now I have to clean and disinfect the keyboard from the Pepsi that I just spewed all over it!üòÇüòÇüòÇ\n",
            "1244412316403277825 2020-03-29 23:53:10 UTC <dabatninjah64> Vacuum cleaners....totally forgot those existed.  Pepsi hiding behind my back with all her claws as a reminder ü§£  pic.twitter.com/BR4yIXIIyW\n",
            "1244412206730620934 2020-03-29 23:52:44 UTC <MAN_4_all_seaso> You know you, mean so much to me! Just like Pepsi, You can ask for more  https://twitter.com/billy_100k/status/1244321754279038978¬†‚Ä¶\n",
            "1244411890954121216 2020-03-29 23:51:29 UTC <KeithManze> A year ago today @JJJetPlane94 and I destroyed the Pepsi Center...and I also left @ThatBoyBeezy_‚Äôs keys in the arena and had to beg security to go back in and get them. Oh and I think the @Avalanche won? #takemeback\n",
            "1244411885778239488 2020-03-29 23:51:28 UTC <duaneonline> @DoctorTro. Maybe Pepsi. Coke cola. Pillsbury. General Mills can funds hospitals. Their customers need ventilators   https://play.google.com/store/books/details?id=lQ6B4hl6ptAC&gl=us&hl=en-US&source=productsearch&utm_source=HA_Desktop_US&utm_medium=SEM&utm_campaign=PLA&pcampaignid=MKTAD0930BO1&gclid=Cj0KCQjwjoH0BRD6ARIsAEWO9DuxshEnI3m56nkGRBLKxrdwE-2O1TpI48otqgmiGkGbubkivCswzecaAnUzEALw_wcB¬†‚Ä¶\n",
            "1244411730920341504 2020-03-29 23:50:51 UTC <KittyMoonPie406> I‚Äôd get a Pepsi.  :)\n",
            "1244411454536597504 2020-03-29 23:49:45 UTC <lombaki> A version of the @pepsi logo I mocked up back in my teens. I'd always hoped they'd go with the 3D ring.  pic.twitter.com/vwZ19W8h0t\n",
            "1244411343991701504 2020-03-29 23:49:19 UTC <katiethebrandt> where was Pepsi\n",
            "1244411310835630081 2020-03-29 23:49:11 UTC <SMarkTweets> I have so much damn Pepsi in my fridge and pantry, 6 packs of bottles have been BOGO for like 3 weeks.\n",
            "1244411271052656641 2020-03-29 23:49:01 UTC <brednewyork> @Frankthetank_k when baseball is back on, I‚Äôm sending you 2 tickets to catch a game at the stadium on me. You will have access to the Audi Club and Pepsi Lounges under my season tickets. üíØ\n",
            "1244411228576940034 2020-03-29 23:48:51 UTC <Zander_Kaiser> 1. Dr Pepper 2. Coke 3. Dew  Pepsi tastes like Jabba the Hutt took a soak in a vat of carbonated water, therefore that disgusting swill does not get to be on this list.  https://twitter.com/jakefromky/status/1244033970339229697¬†‚Ä¶\n",
            "1244411168661389314 2020-03-29 23:48:37 UTC <Kel_GG> How does one have an argument over a Pepsi??? I'm so confused, like what?\n",
            "1244410582842986498 2020-03-29 23:46:17 UTC <DietMikeWitLime> 5 previous jobs you've had. Tag 5 Pepsi Merchandiser Enlisted Soilder Taco Bell Cartpusher at Walmart Lawn Mower Assembly Line @ayebee11 @BarneyDemar @MKonSports @AngieSee21 @susanfisher33 https://twitter.com/toinespeaks/status/1244408637780566017¬†‚Ä¶\n",
            "1244410528149069827 2020-03-29 23:46:04 UTC <RobEdigital> Hope you have enough Pepsi to get you through the apocalypse...  pic.twitter.com/TwF5cyPFJC\n",
            "1244410521991905280 2020-03-29 23:46:03 UTC <thekatiehughey> Best pops.   Cola: Pepsi Diet cola: Diet Coke Lemon-lime: 7UP Root beer: A&W Dr. Pepper: Dr. Pop (Save-a-Lot) Ginger ale: Vernor's Orange soda: Faygo Orange Specialty flavor: Faygo Rock 'n Rye Honorable mention: Faygo Red Pop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjhq7jqbpnHI",
        "colab_type": "text"
      },
      "source": [
        "#Exportation en JSON"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pztS3Ov7DYye",
        "colab_type": "text"
      },
      "source": [
        "## On r√©ctifie les dates"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HeNXYm8ph1O6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Tweets_df['date_jour']=datetime.strptime(Tweets_df['date'][0][:10], ('%Y-%m-%d')).strftime(\"%A\")\n",
        "Tweets_df['date']=Tweets_df['date'][0][:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrPgUH_RDbYG",
        "colab_type": "text"
      },
      "source": [
        "## Pour les nuages de mots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoEoI1zhpk92",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "listeH = []\n",
        "for i in Tweets_df['hashtags'].values:\n",
        "  listeH = listeH+i\n",
        "listeH = Counter(listeH).most_common(50)\n",
        "listeH_arranged = []\n",
        "for i in range(len(listeH)):\n",
        "  listeH_arranged.append({\n",
        "    'name': list(listeH[i])[0],\n",
        "    'weight': list(listeH[i])[1],\n",
        "  })\n",
        "\n",
        "listeM = []\n",
        "for i in Tweets_df['tweet_cleaned'].values:\n",
        "  listeM = listeM+i\n",
        "listeM = Counter(listeM).most_common(50)\n",
        "listeM_arranged = []\n",
        "for i in range(len(listeM)):\n",
        "  listeM_arranged.append({\n",
        "    'name': list(listeM[i])[0],\n",
        "    'weight': list(listeM[i])[1],\n",
        "  })\n",
        "\n",
        "listeE = []\n",
        "for i in Tweets_df['tweet_emojis'].values:\n",
        "  listeE = listeE + i.split()\n",
        "listeE = Counter(listeE).most_common(50)\n",
        "listeE_arranged = []\n",
        "for i in range(len(listeE)):\n",
        "  listeE_arranged.append({\n",
        "    'name': list(listeE[i])[0],\n",
        "    'weight': list(listeE[i])[1],\n",
        "  })"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnNvHUHBvYNG",
        "colab_type": "code",
        "outputId": "de401850-3409-4f6b-cf7d-46a235a29c88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "Tweets_df.groupby('date_jour').mean().nlikes.values"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>created_at</th>\n",
              "      <th>user_id</th>\n",
              "      <th>day</th>\n",
              "      <th>retweet</th>\n",
              "      <th>nlikes</th>\n",
              "      <th>nreplies</th>\n",
              "      <th>nretweets</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>date_jour</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Monday</th>\n",
              "      <td>1.585553e+12</td>\n",
              "      <td>6.009514e+17</td>\n",
              "      <td>3.882353</td>\n",
              "      <td>False</td>\n",
              "      <td>1.647059</td>\n",
              "      <td>0.294118</td>\n",
              "      <td>0.117647</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             created_at       user_id       day  ...    nlikes  nreplies  nretweets\n",
              "date_jour                                        ...                               \n",
              "Monday     1.585553e+12  6.009514e+17  3.882353  ...  1.647059  0.294118   0.117647\n",
              "\n",
              "[1 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 211
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTyTYlf_DgnY",
        "colab_type": "text"
      },
      "source": [
        "## On exporte le tout en JSON"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHtfEVi25dRz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = {}\n",
        "data['pepsi'] = []\n",
        "data['pepsi'].append({\n",
        "    'nbPositif': '',\n",
        "    'nbNegatif': '',\n",
        "    'moyenne': '',\n",
        "    'date': Tweets_df.date.unique().tolist(),\n",
        "    'jour': Tweets_df.date_jour.unique().tolist(),\n",
        "    'nbLikePos': '',\n",
        "    'nbRetweetPos': '',\n",
        "    'nbLikeNeg': '',\n",
        "    'nbRetweetNeg': '',\n",
        "    'hashtagAssocie': listeH_arranged,\n",
        "    'motClefPos': listeM_arranged,\n",
        "    'motClefNeg': listeM_arranged,\n",
        "    'emojis': listeE_arranged\n",
        "})\n",
        "\n",
        "with open('data.json', 'w') as outfile:\n",
        "    json.dump(data, outfile)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WeE0IZPsGkF9",
        "colab_type": "text"
      },
      "source": [
        "#Id√©es"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBKIFhQ3OQ4C",
        "colab_type": "text"
      },
      "source": [
        "##Correction de sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHek8CpNOQIq",
        "colab_type": "code",
        "outputId": "7a7c8a4d-441c-4e24-b776-74c30a139eba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "b = TextBlob(\"I havv goood speling!\")\n",
        "print(b.correct())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I have good spelling!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YoRPZVCoSvZ",
        "colab_type": "text"
      },
      "source": [
        "##Tok√©nisation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IszlYr1oDTl",
        "colab_type": "code",
        "outputId": "7178e657-3e4e-48ec-beed-c549a2471150",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "nltk.pos_tag(['go'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('go', 'VB')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQyMbOQPqWuO",
        "colab_type": "text"
      },
      "source": [
        "##Emotions *par mot*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3q9LKMjSqYjN",
        "colab_type": "code",
        "outputId": "214b148b-a8eb-471b-cd20-83ce08c420c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "from nltk.corpus import sentiwordnet as swn\n",
        "list(swn.senti_synsets('love'))\n",
        "\n",
        "breakdown = swn.senti_synset('kill.v.01')\n",
        "print(\"pos_score: \" , breakdown.pos_score())\n",
        "print(\"neg_score: \" , breakdown.neg_score())\n",
        "print(\"obj_score: \" , breakdown.obj_score())\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pos_score:  0.0\n",
            "neg_score:  0.5\n",
            "obj_score:  0.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3gssa1HmFq3",
        "colab_type": "text"
      },
      "source": [
        "##2eme essai de cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MvbyHZ7C0Lv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize \n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import LancasterStemmer \n",
        "\n",
        "#create an object of class PorterStemmer\n",
        "stop_words = set(stopwords.words('english'))   \n",
        "\n",
        "for i in range(len(Tweets_df)):\n",
        "  word_tokens = word_tokenize(Tweets_df['tweet'][i])   \n",
        "  filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
        "  filtered_sentence = []   \n",
        "  for w in word_tokens: \n",
        "      if w not in stop_words: \n",
        "          filtered_sentence.append(w) \n",
        "  #print(word_tokens) \n",
        "  #print(filtered_sentence)\n",
        "  tweet_rmStop.append(filtered_sentence)\n",
        "  \n",
        "tweet_lemm = tweet_rmStop"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEfDmBTbFHyi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import these modules \n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import LancasterStemmer\n",
        "\n",
        "#create an object of class PorterStemmer\n",
        "porter = PorterStemmer()\n",
        "lancaster=LancasterStemmer()\n",
        "\n",
        "for i in range(len(tweet_rmStop)):\n",
        "  for j in range(len(tweet_rmStop[i])):\n",
        "    tweet_lemm[i][j] = porter.stem(tweet_rmStop[i][j])\n",
        "    print(tweet_lemm[i][j])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUdp7SOEJ62Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas\n",
        "#Tweets_df = pandas.DataFrame()\n",
        "\n",
        "Tweets_df[\"tweet_rmStop\"]= tweet_rmStop\n",
        "Tweets_df[\"tweet_lemm\"] = tweet_lemm\n",
        "Tweets_df[\"tweet_emojis\"] = emojisExtracted"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}