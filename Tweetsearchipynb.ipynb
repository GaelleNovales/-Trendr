{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tweetsearchipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPgEuEMQ3Egkc9NgkPK7/OC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GaelleNovales/Trendr/blob/master/Tweetsearchipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0qJQsAjFpdI",
        "colab_type": "text"
      },
      "source": [
        "#Ressources"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M00xPdlfldlu",
        "colab_type": "text"
      },
      "source": [
        "##Téléchargement des ressources\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ur0DzztRvurI",
        "colab_type": "code",
        "outputId": "f4d3c98c-3f02-4acf-ce39-389ff677d0dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 976
        }
      },
      "source": [
        "!pip install nltk\n",
        "!pip install twint\n",
        "!pip install the Twython library \n",
        "!pip install emoji\n",
        "\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('sentiwordnet')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n",
            "Requirement already satisfied: twint in /usr/local/lib/python3.6/dist-packages (2.1.16)\n",
            "Requirement already satisfied: cchardet in /usr/local/lib/python3.6/dist-packages (from twint) (2.1.6)\n",
            "Requirement already satisfied: pysocks in /usr/local/lib/python3.6/dist-packages (from twint) (1.7.1)\n",
            "Requirement already satisfied: aiodns in /usr/local/lib/python3.6/dist-packages (from twint) (2.0.0)\n",
            "Requirement already satisfied: elasticsearch in /usr/local/lib/python3.6/dist-packages (from twint) (7.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.6/dist-packages (from twint) (3.6.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from twint) (0.25.3)\n",
            "Requirement already satisfied: schedule in /usr/local/lib/python3.6/dist-packages (from twint) (0.6.0)\n",
            "Requirement already satisfied: googletransx in /usr/local/lib/python3.6/dist-packages (from twint) (2.4.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from twint) (4.6.3)\n",
            "Requirement already satisfied: geopy in /usr/local/lib/python3.6/dist-packages (from twint) (1.17.0)\n",
            "Requirement already satisfied: aiohttp-socks in /usr/local/lib/python3.6/dist-packages (from twint) (0.3.6)\n",
            "Requirement already satisfied: fake-useragent in /usr/local/lib/python3.6/dist-packages (from twint) (0.1.11)\n",
            "Requirement already satisfied: typing; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from aiodns->twint) (3.6.6)\n",
            "Requirement already satisfied: pycares>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from aiodns->twint) (3.1.1)\n",
            "Requirement already satisfied: urllib3>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from elasticsearch->twint) (1.24.3)\n",
            "Requirement already satisfied: chardet<4.0,>=2.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->twint) (3.0.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->twint) (19.3.0)\n",
            "Requirement already satisfied: async-timeout<4.0,>=3.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->twint) (3.0.1)\n",
            "Requirement already satisfied: multidict<5.0,>=4.5 in /usr/local/lib/python3.6/dist-packages (from aiohttp->twint) (4.7.5)\n",
            "Requirement already satisfied: typing-extensions>=3.6.5; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from aiohttp->twint) (3.6.6)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->twint) (1.4.2)\n",
            "Requirement already satisfied: idna-ssl>=1.0; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from aiohttp->twint) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from pandas->twint) (1.18.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->twint) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->twint) (2.8.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from googletransx->twint) (2.21.0)\n",
            "Requirement already satisfied: geographiclib<2,>=1.49 in /usr/local/lib/python3.6/dist-packages (from geopy->twint) (1.50)\n",
            "Requirement already satisfied: cffi>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pycares>=3.0.0->aiodns->twint) (1.14.0)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.6/dist-packages (from yarl<2.0,>=1.0->aiohttp->twint) (2.8)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas->twint) (1.12.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->googletransx->twint) (2019.11.28)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=1.5.0->pycares>=3.0.0->aiodns->twint) (2.20)\n",
            "Requirement already satisfied: the in /usr/local/lib/python3.6/dist-packages (0.1.5)\n",
            "Requirement already satisfied: Twython in /usr/local/lib/python3.6/dist-packages (3.7.0)\n",
            "Requirement already satisfied: library in /usr/local/lib/python3.6/dist-packages (0.0.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from Twython) (1.3.0)\n",
            "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from Twython) (2.21.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.4.0->Twython) (3.1.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.1.0->Twython) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.1.0->Twython) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.1.0->Twython) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.1.0->Twython) (2019.11.28)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.6/dist-packages (0.5.4)\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package sentiwordnet to /root/nltk_data...\n",
            "[nltk_data]   Package sentiwordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqQIoDNbFt34",
        "colab_type": "text"
      },
      "source": [
        "##Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZ53eN4lFwiS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import twint\n",
        "from datetime import datetime\n",
        "import emoji\n",
        "import regex\n",
        "import re\n",
        "from nltk import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import WordNetLemmatizer\n",
        "import pandas as pd\n",
        "from textblob import TextBlob\n",
        "import csv\n",
        "from datetime import date, datetime\n",
        "from collections import Counter\n",
        "import json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phEOj3fplrft",
        "colab_type": "text"
      },
      "source": [
        "#Création du projet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hg1KSWr6li7C",
        "colab_type": "text"
      },
      "source": [
        "## Importation des tweets selon un hashtag\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmZfM-Cw8pTJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def export_tweets(mention,taille):\n",
        "  date = datetime.now()\n",
        "  date_format = str(date.year)+'-'+str(date.month)+'-'+str(date.day)\n",
        "\n",
        "  # Configure\n",
        "  c = twint.Config()\n",
        "  c.Search = mention\n",
        "  c.Store_csv = True\n",
        "  c.Output = c.Search+\"_saved.csv\"\n",
        "  c.Limit = taille\n",
        "  c.Since = date_format\n",
        "  c.Lang = 'en'\n",
        "  c.Pandas = True\n",
        "\n",
        "  # Run\n",
        "  twint.run.Search(c)\n",
        "\n",
        "  twint_data = twint.storage.panda.Tweets_df\n",
        "  return twint_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4l7hS7Cl0yU",
        "colab_type": "text"
      },
      "source": [
        "##Extraction des emojis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xH2BnijOHOpT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_emojis(a_list):\n",
        "    emojis_list = map(lambda x: ''.join(x.split()), emoji.UNICODE_EMOJI.keys())\n",
        "    r = re.compile('|'.join(re.escape(p) for p in emojis_list))\n",
        "    aux=[' '.join(r.findall(s)) for s in a_list]\n",
        "    return(aux)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKUvgG6DI4f5",
        "colab_type": "text"
      },
      "source": [
        "##Traduire les abréviations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6j8mK4uHHjnW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translator(user_string):\n",
        "    user_string = user_string.split(\" \")\n",
        "    j = 0\n",
        "    for _str in user_string:\n",
        "        # File path which consists of Abbreviations.\n",
        "        fileName = \"slang.txt\"\n",
        "        # File Access mode [Read Mode]\n",
        "        accessMode = \"r\"\n",
        "        with open(fileName, accessMode) as myCSVfile:\n",
        "            # Reading file as CSV with delimiter as \"=\", so that abbreviation are stored in row[0] and phrases in row[1]\n",
        "            dataFromFile = csv.reader(myCSVfile, delimiter=\"=\")\n",
        "            # Removing Special Characters.\n",
        "            _str = re.sub('[^a-zA-Z0-9-_.]', '', _str)\n",
        "            for row in dataFromFile:\n",
        "                # Check if selected word matches short forms[LHS] in text file.\n",
        "                if _str.upper() == row[0]:\n",
        "                    # If match found replace it with its appropriate phrase in text file.\n",
        "                    user_string[j] = row[1]\n",
        "            myCSVfile.close()\n",
        "        j = j + 1\n",
        "    # Replacing commas with spaces for final output.\n",
        "    return (' '.join(user_string))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aVmkbRAl6x_",
        "colab_type": "text"
      },
      "source": [
        "##Cleaning du texte"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHU8nxJ5jhbO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_text(text,mention):\n",
        "    \"\"\"\n",
        "    This function takes as input a text on which several \n",
        "    NLTK algorithms will be applied in order to preprocess it\n",
        "    \"\"\"\n",
        "    # Remplacer les abréviations\n",
        "    #print(text)\n",
        "    #print(translator(text))\n",
        "    text = translator(text)\n",
        "\n",
        "    tokens = word_tokenize(text)\n",
        "      \n",
        "    # Remove the punctuations\n",
        "    tokens = [word for word in tokens if word.isalpha()]\n",
        "    # Lower the tokens\n",
        "    tokens = [word.lower() for word in tokens]\n",
        "\n",
        "    # Remove stopword\n",
        "    tokens = [word for word in tokens if not word in stopwords.words(\"english\")]\n",
        "\n",
        "    # Corriger les fautes\n",
        "    #for word in tokens:\n",
        "    #  print(word)\n",
        "    #  print(str(TextBlob(word).correct()))\n",
        "    tokens = [str(TextBlob(word).correct()) for word in tokens if word != mention]\n",
        "\n",
        "    # Lemmatize\n",
        "    lemma = WordNetLemmatizer()\n",
        "    tokens = [lemma.lemmatize(word, pos = \"v\") for word in tokens]\n",
        "    tokens = [lemma.lemmatize(word, pos = \"n\") for word in tokens]\n",
        "    return tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Vxy-dldGmkm",
        "colab_type": "text"
      },
      "source": [
        "#Execution du programme"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dyqn76KUGiQ8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prog(mention,taille):\n",
        "  Tweets_df = export_tweets(mention,taille)\n",
        "\n",
        "  tweet_cleaned = []\n",
        "  for i in range(len(Tweets_df)):\n",
        "    tweet_cleaned.append(clean_text(Tweets_df['tweet'][i],mention))  \n",
        "\n",
        "  emojisExtracted = extract_emojis(Tweets_df['tweet'])\n",
        "  Tweets_df[\"tweet_emojis\"] = emojisExtracted\n",
        "  Tweets_df[\"tweet_cleaned\"] = tweet_cleaned\n",
        "  return Tweets_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMG5fRSmIEVh",
        "colab_type": "code",
        "outputId": "cea91e89-9c2d-4aee-b312-984923f10c59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        }
      },
      "source": [
        "Tweets_df = prog(\"#pepsi\",100)\n",
        "#df.drop('column_name', axis=1, inplace=True)"
      ],
      "execution_count": 230,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1244634481367093248 2020-03-30 14:35:59 UTC <EbbaQ> Love it when the big giants come together to help the entire nation. Thank you #Pepsi !  https://twitter.com/pepsipakistan/status/1244611497403338752 …\n",
            "1244631321671921670 2020-03-30 14:23:25 UTC <warisha_rais> What a brilliant initiative and I hope more details would be unveiled soon so we all can play our part in supporting our people. #pepsi  https://twitter.com/pepsipakistan/status/1244611497403338752 …\n",
            "1244617488245145601 2020-03-30 13:28:27 UTC <doberhoocom> #pepsi SlowGuns - The Dober Hoo Stuff  Thousand pages for amazing book  pic.twitter.com/0Fexu6DQT9\n",
            "1244609224195399683 2020-03-30 12:55:37 UTC <carbonateddave> Pepsi drinkers are the queen of the playground! #pepsi\n",
            "1244609192943669248 2020-03-30 12:55:29 UTC <selzyboy> @wizkidayo as I like you reach. You have not reached out to us #wizkidfc on how you’re helping. Give us money to go and buy #PEPSI @pepsi naaa  I don’t know sha. Some music would heal us\n",
            "1244596620362039296 2020-03-30 12:05:32 UTC <misugihironori> I drew a pepsi cola.The making video will be up soon. #pepsi #pepsicola #drawing #draw #Illustrations #illustration #illust #3dart #絵描きさんと繫がりたい #アナログ絵描きさんと繋がりたい #イラスト好きさんと繋がりたい pic.twitter.com/4IVuuCXgsq\n",
            "1244544559020789760 2020-03-30 08:38:40 UTC <Tickeron> $PEP's price moved below its 50-day Moving Average on March 5, 2020. View odds for this and other indicators:  https://tickeron.com/go/1424125  #Pepsi #stockmarket #stock #technicalanalysis #money #trading #investing #daytrading #news #today pic.twitter.com/doSODDNZe1\n",
            "1244524524680478722 2020-03-30 07:19:03 UTC <HassanB40146545> Eat well and stay safe     #PEPSI  pic.twitter.com/e7Em54BntT\n",
            "1244518525739376640 2020-03-30 06:55:13 UTC <carbonateddave> Pepsi drinkers are the cool kids of the stadium! #pepsi\n",
            "1244510760199262210 2020-03-30 06:24:21 UTC <ScotsmanGrumpy> This is what the after-stream fallout looks like in the Grumpy Household 🥤 #TwitchStreamers #Pepsi  pic.twitter.com/bvgKBcUY0P\n",
            "1244502827009036289 2020-03-30 05:52:50 UTC <DavidCHelms1> But drinking #Pepsi\n",
            "1244496892677029888 2020-03-30 05:29:15 UTC <Hallucinatier> #Pepsi makes me smile : )  pic.twitter.com/lCwGjjZJR6\n",
            "1244496172443504642 2020-03-30 05:26:23 UTC <Jennnesssa> So good I had to share! Check out all the items I'm loving on @Poshmarkapp from @vlynn2016 #poshmark #fashion #style #shopmycloset #rosetti #pepsi:  https://posh.mk/bT4bMmCSf3  pic.twitter.com/kvkujDYR93\n",
            "1244477892693610496 2020-03-30 04:13:45 UTC <nonobject> Working on refining it but wanted to get this out ASAP as it could help thousands of people. Making this mask takes about 2-3 min. #NYTimes #WashingtonPost #Covid_19 #PEPSI #cocacola\n",
            "1244465919889305601 2020-03-30 03:26:11 UTC <ShiMarie24> So good I had to share! Check out all the items I'm loving on @Poshmarkapp from @brookeu123 @Brandon00643690 #poshmark #fashion #style #shopmycloset #merona #pepsi:  https://posh.mk/83HbJDizg5  pic.twitter.com/68vLuMsGF5\n",
            "1244453508088311808 2020-03-30 02:36:51 UTC <NPC_Anthony> Mom: Diet cokes are unhealthy. Also Mom: Fried chicken for dinner. #cocacola #PEPSI #KFC\n",
            "1244451993445224448 2020-03-30 02:30:50 UTC <Myself16715252> #tropicana #PEPSI #pepsico #Gurgaon I thank god for privileges But There is no sealing on inside 2 tetra packs.  Pls check the batch code and possibility of adulteration and poor preservation .  Is it not going to make people more sick. pic.twitter.com/wTpZbtqZ5b\n",
            "1244427880194605056 2020-03-30 00:55:01 UTC <carbonateddave> Pepsi drinkers are the queen of the party! #pepsi\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjhq7jqbpnHI",
        "colab_type": "text"
      },
      "source": [
        "#Exportation en JSON"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pztS3Ov7DYye",
        "colab_type": "text"
      },
      "source": [
        "## On réctifie les dates"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HeNXYm8ph1O6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Tweets_df['date_jour']=datetime.strptime(Tweets_df['date'][0][:10], ('%Y-%m-%d')).strftime(\"%A\")\n",
        "Tweets_df['date']=Tweets_df['date'][0][:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrPgUH_RDbYG",
        "colab_type": "text"
      },
      "source": [
        "## Pour les nuages de mots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoEoI1zhpk92",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "listeH = []\n",
        "for i in Tweets_df['hashtags'].values:\n",
        "  listeH = listeH+i\n",
        "listeH = Counter(listeH).most_common(50)\n",
        "\n",
        "listeM = []\n",
        "for i in Tweets_df['tweet_cleaned'].values:\n",
        "  listeM = listeM+i\n",
        "listeM = Counter(listeM).most_common(50)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnNvHUHBvYNG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "de401850-3409-4f6b-cf7d-46a235a29c88"
      },
      "source": [
        "Tweets_df.groupby('date_jour').mean().nlikes.values"
      ],
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>created_at</th>\n",
              "      <th>user_id</th>\n",
              "      <th>day</th>\n",
              "      <th>retweet</th>\n",
              "      <th>nlikes</th>\n",
              "      <th>nreplies</th>\n",
              "      <th>nretweets</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>date_jour</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Monday</th>\n",
              "      <td>1.585553e+12</td>\n",
              "      <td>6.009514e+17</td>\n",
              "      <td>3.882353</td>\n",
              "      <td>False</td>\n",
              "      <td>1.647059</td>\n",
              "      <td>0.294118</td>\n",
              "      <td>0.117647</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             created_at       user_id       day  ...    nlikes  nreplies  nretweets\n",
              "date_jour                                        ...                               \n",
              "Monday     1.585553e+12  6.009514e+17  3.882353  ...  1.647059  0.294118   0.117647\n",
              "\n",
              "[1 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 211
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTyTYlf_DgnY",
        "colab_type": "text"
      },
      "source": [
        "## On exporte le tout en JSON"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHtfEVi25dRz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = {}\n",
        "data['pepsi'] = []\n",
        "data['pepsi'].append({\n",
        "    'nbPositif': '',\n",
        "    'nbNegatif': '',\n",
        "    'moyenne': '',\n",
        "    'date': Tweets_df.date.unique().tolist(),\n",
        "    'jour': Tweets_df.date_jour.unique().tolist(),\n",
        "    'nbLikePos': '',\n",
        "    'nbRetweetPos': '',\n",
        "    'nbLikeNeg': '',\n",
        "    'nbRetweetNeg': '',\n",
        "    'hastagAssocie': listeH,\n",
        "    'motClefPos': listeM,\n",
        "    'motClefNeg': listeM\n",
        "})\n",
        "\n",
        "with open('data.json', 'w') as outfile:\n",
        "    json.dump(data, outfile)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WeE0IZPsGkF9",
        "colab_type": "text"
      },
      "source": [
        "#Idées"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBKIFhQ3OQ4C",
        "colab_type": "text"
      },
      "source": [
        "##Correction de sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHek8CpNOQIq",
        "colab_type": "code",
        "outputId": "7a7c8a4d-441c-4e24-b776-74c30a139eba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "b = TextBlob(\"I havv goood speling!\")\n",
        "print(b.correct())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I have good spelling!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YoRPZVCoSvZ",
        "colab_type": "text"
      },
      "source": [
        "##Tokénisation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IszlYr1oDTl",
        "colab_type": "code",
        "outputId": "7178e657-3e4e-48ec-beed-c549a2471150",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "nltk.pos_tag(['go'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('go', 'VB')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQyMbOQPqWuO",
        "colab_type": "text"
      },
      "source": [
        "##Emotions *par mot*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3q9LKMjSqYjN",
        "colab_type": "code",
        "outputId": "214b148b-a8eb-471b-cd20-83ce08c420c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "from nltk.corpus import sentiwordnet as swn\n",
        "list(swn.senti_synsets('love'))\n",
        "\n",
        "breakdown = swn.senti_synset('kill.v.01')\n",
        "print(\"pos_score: \" , breakdown.pos_score())\n",
        "print(\"neg_score: \" , breakdown.neg_score())\n",
        "print(\"obj_score: \" , breakdown.obj_score())\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pos_score:  0.0\n",
            "neg_score:  0.5\n",
            "obj_score:  0.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3gssa1HmFq3",
        "colab_type": "text"
      },
      "source": [
        "##2eme essai de cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MvbyHZ7C0Lv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize \n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import LancasterStemmer \n",
        "\n",
        "#create an object of class PorterStemmer\n",
        "stop_words = set(stopwords.words('english'))   \n",
        "\n",
        "for i in range(len(Tweets_df)):\n",
        "  word_tokens = word_tokenize(Tweets_df['tweet'][i])   \n",
        "  filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
        "  filtered_sentence = []   \n",
        "  for w in word_tokens: \n",
        "      if w not in stop_words: \n",
        "          filtered_sentence.append(w) \n",
        "  #print(word_tokens) \n",
        "  #print(filtered_sentence)\n",
        "  tweet_rmStop.append(filtered_sentence)\n",
        "  \n",
        "tweet_lemm = tweet_rmStop"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEfDmBTbFHyi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import these modules \n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import LancasterStemmer\n",
        "\n",
        "#create an object of class PorterStemmer\n",
        "porter = PorterStemmer()\n",
        "lancaster=LancasterStemmer()\n",
        "\n",
        "for i in range(len(tweet_rmStop)):\n",
        "  for j in range(len(tweet_rmStop[i])):\n",
        "    tweet_lemm[i][j] = porter.stem(tweet_rmStop[i][j])\n",
        "    print(tweet_lemm[i][j])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUdp7SOEJ62Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas\n",
        "#Tweets_df = pandas.DataFrame()\n",
        "\n",
        "Tweets_df[\"tweet_rmStop\"]= tweet_rmStop\n",
        "Tweets_df[\"tweet_lemm\"] = tweet_lemm\n",
        "Tweets_df[\"tweet_emojis\"] = emojisExtracted"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}